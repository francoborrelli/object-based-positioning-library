# Object Based Positioning Library

![React Native](https://img.shields.io/badge/react_native-%2320232a.svg?style=for-the-badge&logo=react&logoColor=%2361DAFB)  ![Expo](https://img.shields.io/badge/expo-1C1E24?style=for-the-badge&logo=expo&logoColor=#D04A37) ![TensorFlow](https://img.shields.io/badge/TensorFlow-%23FF6F00.svg?style=for-the-badge&logo=TensorFlow&logoColor=white) ![TypeScript](https://img.shields.io/badge/typescript-%23007ACC.svg?style=for-the-badge&logo=typescript&logoColor=white)

> [!NOTE]
> **Summary:** Prototype mechanism for achieving user positioning in indoor spaces using object recognition and detection models. This library works exclusively with **React Native** and **Expo**. A concrete implementation using **Firebase** to store data is presented, although other mechanisms could be used.


## ðŸŒŸ Introduction

This library is presented as a concrete implementation aimed at enabling a proof of concept. The library allows for positioning users in indoor spaces using various object recognition models. To achieve positioning, there are two stages involved. In the first stage, the user needs to use their phone's camera to scan the environment, recording different objects present in the space. These "points of interest" can then be accessed in the second stage, providing contextual information or services to other users.

## ðŸ“š Table of Contents

1. ðŸ“– [Documentation](#documentation)

## ðŸ“– Documentation
